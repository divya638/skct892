1(a)-Hadoop install
•	sudo su
•	sudo yum update -y
•	sudo yum install java-11-openjdk-devel -y or sudo yum install java-1.8.0-amazon-corretto- devel -y
•	java -version
•	cd /usr/local/
•	sudo wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
•	sudo tar -xvzf hadoop-3.3.6.tar.gz
•	sudo mv hadoop-3.3.6 hadoop
•	sudo nano ~/.bashrc
o	# Hadoop variables
o	export HADOOP_HOME=/usr/local/hadoop
o	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
o	# Java variables
o	export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:/bin/java::")
o	export PATH=$PATH:$JAVA_HOME/bin
•	source ~/.bashrc
•	hadoop version
1(b)-Hadoop modes
•	sudo su
•	sudo yum update -y
•	sudo yum install java-11-openjdk-devel -y or sudo yum install java-1.8.0-amazon-corretto- devel -y
•	java -version
•	cd /usr/local/
•	sudo wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
•	sudo tar -xvzf hadoop-3.3.6.tar.gz
•	sudo mv hadoop-3.3.6 hadoop
 
•	sudo nano ~/.bashrc
o	# Hadoop variables
o	export HADOOP_HOME=/usr/local/hadoop
o	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
o	# Java variables
o	export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:/bin/java::")
o	export PATH=$PATH:$JAVA_HOME/bin
•	source ~/.bashrc
•	hadoop version
•	Configure core-site.xml
o	sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
o	<configuration>
o	<property>
o	<name>fs.defaultFS</name>
o	<value>hdfs://localhost:9000</value>
o	</property>
o	<property>
o	<name>hadoop.tmp.dir</name>
o	<value>/usr/local/hadoop/tmp</value>
o	<description>Temporary directory for Hadoop</description>
o	</property>
o	</configuration>
•	Configure hdfs-site.xml
o	sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
o	<configuration>
o	<property>
o	<name>dfs.replication</name>
o	<value>1</value> <!-- Since this is a single-node setup -->
o	</property>
 
o	<property>
o	<name>dfs.namenode.name.dir</name>
o	<value>file:///usr/local/hadoop/hdfs/namenode</value>
o	</property>
o	<property>
o	<name>dfs.datanode.data.dir</name>
o	<value>file:///usr/local/hadoop/hdfs/datanode</value>
o	</property>
o	</configuration>
•	Create the mapred-site.xml[If the mapred-site.xml.template is not present]
o	sudo nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
o	<configuration>
o	<property>
o	<name>mapreduce.framework.name</name>
o	<value>yarn</value>
o	</property>
o	</configuration>
•	Configure yarn-site.xml
o	sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
o	<configuration>
o	<property>
o	<name>yarn.nodemanager.aux-services</name>
o	<value>mapreduce_shuffle</value>
o	</property>
o	<property>
o	<name>yarn.resourcemanager.resource-tracker.address</name>
o	<value>localhost:8025</value>
o	</property>
o	<property>
 
o	<name>yarn.resourcemanager.scheduler.address</name>
o	<value>localhost:8030</value>
o	</property>
o	<property>
o	<name>yarn.resourcemanager.address</name>
o	<value>localhost:8050</value>
o	</property>
o	</configuration>
•	Set Up Hadoop Directories
o	sudo mkdir -p /usr/local/hadoop/hdfs/namenode
o	sudo mkdir -p /usr/local/hadoop/hdfs/datanode
o	sudo mkdir -p /usr/local/hadoop/tmp
•	Format the HDFS Filesystem
o	hdfs namenode -format
•	Start Hadoop Services(As Non - Root User)
o	start-dfs.sh
o	start-yarn.sh
o	Jps
•	Access the Hadoop Web Interfaces
o	NameNode: http://localhost:9870/ (shows the HDFS overview)
o	ResourceManager: http://localhost:8088/ (shows the YARN overview)
o	LocalHost - 127.0.0.1 or public DNS :- 3.117.182.16\

2-hdfs
•	sudo su
•	hdfs dfs -put /local/path/to/file /hdfs/path/
•	hdfs dfs -ls /hdfs/path/
•	hdfs dfs -mkdir /hdfs/path/
•	hdfs dfs -get /hdfs/path/to/file /local/path/
•	hdfs dfs -rm /hdfs/path/to/file
•	hdfs dfs -rm -r /hdfs/path/to/directory
•	hdfs dfs -cat /hdfs/path/to/file
•	hdfs dfs -du -h /hdfs/path/
•	hdfs dfs -cp /hdfs/source/path /hdfs/destination/path 

3-mr-matrix
•	sudo su
•	MatrixMapper.java
o	import java.io.IOException;
o	import org.apache.hadoop.io.IntWritable;
o	import org.apache.hadoop.io.Text;
o	import org.apache.hadoop.mapreduce.Mapper;
o	public class MatrixMapper extends Mapper<Object, Text, Text, Text> {
o	@Override
o		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
o	String[] line = value.toString().split(",");
o	String matrixName = line[0]; // A or B
o	int i = Integer.parseInt(line[1]);
o	int j = Integer.parseInt(line[2]);
o	int valueOfElement = Integer.parseInt(line[3]);
 

o	if (matrixName.equals("A")) {
o	// Emit for all columns of B
o	for (int k = 0; k < context.getConfiguration().getInt("p", 0); k++) {
o		context.write(new Text(i + "," + k), new Text("A," + j + "," + valueOfElement));
o	}
o	} else {
o	// Emit for all rows of A
o	for (int k = 0; k < context.getConfiguration().getInt("m", 0); k++) {
o		context.write(new Text(k + "," + j), new Text("B," + i + "," + valueOfElement));
o	}
o	}
o	}
o	}
•	MatrixReducer.java
o	import java.io.IOException;
o	import java.util.HashMap;
o	import java.util.Map;
o	import org.apache.hadoop.io.IntWritable;
o	import org.apache.hadoop.io.Text;
o	import org.apache.hadoop.mapreduce.Reducer;
o	public class MatrixReducer extends Reducer<Text, Text, Text, IntWritable> {
o	@Override
o		public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
o	Map<Integer, Integer> mapA = new HashMap<>();
 
o	Map<Integer, Integer> mapB = new HashMap<>();
o	for (Text val : values) {
o	String[] parts = val.toString().split(",");
o	if (parts[0].equals("A")) {
o	mapA.put(Integer.parseInt(parts[1]), Integer.parseInt(parts[2]));
o	} else {
o	mapB.put(Integer.parseInt(parts[1]), Integer.parseInt(parts[2]));
o	}
o	}
o	int result = 0;
o	for (Integer k : mapA.keySet()) {
o	if (mapB.containsKey(k)) {
o	result += mapA.get(k) * mapB.get(k);
o	}
o	}

o	context.write(key, new IntWritable(result));
o	}
o	}
•	MatrixMultiplication.java
o	import org.apache.hadoop.conf.Configuration;
o	import org.apache.hadoop.fs.Path;
o	import org.apache.hadoop.io.IntWritable;
o	import org.apache.hadoop.io.Text;
o	import org.apache.hadoop.mapreduce.Job;
o	import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
o	import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 
o	public class MatrixMultiplication {
o	public static void main(String[] args) throws Exception {
o	Configuration conf = new Configuration();
o	// Dimensions of the matrices A (m x n) and B (n x p)
o	conf.setInt("m", 3); // Rows of A
o	conf.setInt("n", 2); // Columns of A and Rows of B
o	conf.setInt("p", 3); // Columns of B
o	Job job = Job.getInstance(conf, "Matrix Multiplication");
o	job.setJarByClass(MatrixMultiplication.class);
o	job.setMapperClass(MatrixMapper.class);
o	job.setReducerClass(MatrixReducer.class);
o	
o	job.setOutputKeyClass(Text.class);
o	job.setOutputValueClass(IntWritable.class);
o	FileInputFormat.addInputPath(job, new Path(args[0]));
o	FileOutputFormat.setOutputPath(job, new Path(args[1]));
o	System.exit(job.waitForCompletion(true) ? 0 : 1);
o	}
o	}
•	Steps to Run the Code
o	hdfs dfs -put matrixA.txt /input/
o	hdfs dfs -put matrixB.txt /input/
o	hadoop com.sun.tools.javac.Main MatrixMultiplication.java
o	jar cf matrixmultiplication.jar MatrixMultiplication*.class
o	hadoop jar matrixmultiplication.jar MatrixMultiplication /input/ /output/
o	hdfs dfs -cat /output/part-r-00000 

4-mr-word count
•	sudo su
•	WordCountMapper.java
o	import java.io.IOException;
o	import org.apache.hadoop.io.IntWritable;
o	import org.apache.hadoop.io.LongWritable;
o	import org.apache.hadoop.io.Text;
o	import org.apache.hadoop.mapreduce.Mapper;
o	public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
o	private final static IntWritable one = new IntWritable(1);
o	private Text word = new Text();
o	@Override
o		protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
o	String[] words = value.toString().split("\\s+");
o	for (String str : words) {
 
o	word.set(str.replaceAll("[^a-zA-Z]", "").toLowerCase()); // Normalize word
o	if (!word.toString().isEmpty()) {
o	context.write(word, one);
o	}
o	}
o	}
o	}
•	WordCountReducer.java
o	import java.io.IOException;
o	import org.apache.hadoop.io.IntWritable;
o	import org.apache.hadoop.io.Text;
o	import org.apache.hadoop.mapreduce.Reducer;
o	public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>
{
o	@Override
o		protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
o	int sum = 0;
o	for (IntWritable val : values) {
o	sum += val.get();
o	}
o	context.write(key, new IntWritable(sum));
o	}
o	}

•	WordCount.java
o	import org.apache.hadoop.conf.Configuration;
o	import org.apache.hadoop.fs.Path;
o	import org.apache.hadoop.io.IntWritable;
o	import org.apache.hadoop.io.Text;
o	import org.apache.hadoop.mapreduce.Job;
o	import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 
o	import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
o	public class WordCount {
o	public static void main(String[] args) throws Exception {
o	Configuration conf = new Configuration();
o	Job job = Job.getInstance(conf, "Word Count");
o	job.setJarByClass(WordCount.class);
o	job.setMapperClass(WordCountMapper.class);
o	job.setReducerClass(WordCountReducer.class);
o	job.setOutputKeyClass(Text.class);
o	job.setOutputValueClass(IntWritable.class);
o	FileInputFormat.addInputPath(job, new Path(args[0]));
o	FileOutputFormat.setOutputPath(job, new Path(args[1]));
o	System.exit(job.waitForCompletion(true) ? 0 : 1);
o	}
o	}

•	Steps to Run the Code
o	hadoop com.sun.tools.javac.Main WordCount.java
o	jar cf wordcount.jar WordCount*.class
o	hadoop jar wordcount.jar WordCount /input /output
o	hdfs dfs -cat /output/part-r-00000

5-hive install
•	sudo su
•	Update the system
o	sudo yum update -y
•	Install wget
o	sudo yum install wget -y
•	Download Apache Hive
o	wget https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz
•	Extract the Hive tarball
o	tar -xzvf apache-hive-3.1.3-bin.tar.gz
•	Move to root directory
o	sudo mv apache-hive-3.1.3-bin /usr/local/hive
•	Configure Hive Environment Variables
o	nano ~/.bashrc
o	Add the following in that file as #Hive Variables
	export HIVE_HOME=/usr/local/hive
	export PATH=$PATH:$HIVE_HOME/bin
	export HADOOP_HOME=/usr/local/hadoop # Adjust to your Hadoop installation path
	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
o	source ~/.bashrc

6(a)-hbase install
•	sudo su
•	Update the system
o	sudo yum update -y
•	Install wget
o	sudo yum install wget -y
•	Download HBase
o	wget https://downloads.apache.org/hbase/2.4.15/hbase-2.4.15-bin.tar.gz
•	Extract the HBase tarball
o	tar -xzvf hbase-2.4.15-bin.tar.gz
•	Move to root directory
o	sudo mv hbase-2.4.15 /usr/local/hbase
•	Configure Hbase Environment Variables
o	nano ~/.bashrc
o	Add the following in that file as #HBase Variables
	export HBASE_HOME=/usr/local/hbase
	export PATH=$PATH:$HBASE_HOME/bin
	export JAVA_HOME=/usr/lib/jvm/java-1.8.0source ~/.bashrc
o	source ~/.bashrc
•	Configure HBase
o	cd /usr/local/hbase/conf
o	sudo nano hbase-env.sh
o	export JAVA_HOME=/usr/lib/jvm/java-1.8.0
•	Configure HBase storage mode
o	sudo nano hbase-site.xml
	<configuration>
	<property>
	<name>hbase.rootdir</name>
 
	<value>file:///usr/local/hbase/data</value>
	</property>
	<property>
	<name>hbase.zookeeper.property.dataDir</name>
	<value>/usr/local/hbase/zookeeper</value>
	</property>
	</configuration>
•	start-hbase.sh
6(b)-thrift
•	sudo su
•	# Start HBase shell
•	hbase shell
•	# Create a table
•	create 'my_table', 'my_column_family'
•	# Insert data (Create)
•	put 'my_table', 'row1', 'my_column_family:name', 'Alice'
•	put 'my_table', 'row1', 'my_column_family:age', '30'
•	put 'my_table', 'row2', 'my_column_family:name', 'Bob'
•	put 'my_table', 'row2', 'my_column_family:age', '25'
•	# Read data (Retrieve)
•	get 'my_table', 'row1'
•	scan 'my_table'
•	# Update data
•	put 'my_table', 'row1', 'my_column_family:age', '31'
•	# Delete data
•	delete 'my_table', 'row1', 'my_column_family:age'
•	deleteall 'my_table', 'row1' # Deletes entire row1
•	# Drop the table (optional)
•	drop 'my_table'

7-import n export
•	sudo su
•	# Part 1: Import Data from MySQL to Hive
o	# Check Sqoop Installation
o	sqoop version
o	# Check MySQL Schema and Table Structure
o	mysql -u your_user -p
o	DESCRIBE your_database.your_table;
o	# Import MySQL Data into Hive using Sqoop
o	sqoop import \
o	--connect jdbc:mysql://your-mysql-server-ip:3306/your_database \
o	--username your_user \
o	--password your_password \
o	--table your_table \
o	--hive-import \
o	--hive-table your_hive_database.your_hive_table \
o	--m 1
 
o	# Verify the Hive Table
o	hive
o	DESCRIBE your_hive_database.your_hive_table;
o	SELECT * FROM your_hive_database.your_hive_table LIMIT 10;
•	# Part 2: Export Data from Hive to MySQL
o	# Check Hive Table Structure
o	DESCRIBE your_hive_database.your_hive_table;
o	# Export Hive Data to MySQL
o	sqoop export \
o	--connect jdbc:mysql://your-mysql-server-ip:3306/your_database \
o	--username your_user \
o	--password your_password \
o	--table your_table \
o	--export-dir /user/hive/warehouse/your_hive_table \
o	--m 1
o	# Check the Exported Data in MySQL
o	SELECT * FROM your_database.your_table LIMIT 10;

8-hive index
•	sudo su
•	-- Step 1: Start Hive
o	hive
•	-- Step 2: Create a Hive Table
o	CREATE TABLE employees (
o	id INT,
o	name STRING,
o	age INT,
o	department STRING
o	) ROW FORMAT DELIMITED
o	FIELDS TERMINATED BY ','
o	STORED AS TEXTFILE;
•	-- Step 3: Load Data into the Table
o	LOAD DATA LOCAL INPATH '/path/to/your/employees.csv' INTO TABLE employees;
•	-- Step 4: Create an Index on the Table
o	CREATE INDEX idx_department
o	ON TABLE employees (department)
o	AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
o	WITH DEFERRED REBUILD;
 
•	-- Step 5: Rebuild the Index
o	ALTER INDEX idx_department ON employees REBUILD;
•	-- Step 6: Query the Table Using the Index
o	SELECT * FROM employees WHERE department = 'Sales';
•	-- Step 7: Drop the Index (Optional)
o	DROP INDEX idx_department ON employees;



9-hive views
•	sudo su
•	-- Step 1: Start Hive
o	hive
•	-- Step 2: Create a Hive Table
o	CREATE TABLE employees (
o	id INT,
o	name STRING,
o	age INT,
o	department STRING
o	) ROW FORMAT DELIMITED
o	FIELDS TERMINATED BY ','
o	STORED AS TEXTFILE;
•	-- Step 3: Load Data into the Table
o	LOAD DATA LOCAL INPATH '/path/to/your/employees.csv' INTO TABLE employees;
•	-- Step 4: Create a View
o	CREATE VIEW sales_employees AS
o	SELECT id, name, age
o	FROM employees
o	WHERE department = 'Sales';
 
•	-- Step 5: Query the View
o	SELECT * FROM sales_employees;
•	-- Step 6: Update the View (Optional)
o	DROP VIEW sales_employees;
o	CREATE VIEW sales_employees AS
o	SELECT id, name, age, department
o	FROM employees
o	WHERE department = 'Sales';
•	-- Step 7: Drop the View (Optional)
o	DROP VIEW sales_employees;

10-external table
 Step 1: Creating a Table
CREATE EXTERNAL TABLE
student_external ( name
string, class ARRAY,
gender_age STRUCT, subj_score MAP
)
COMMENT ' External student table' ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' STORED AS TEXTFILE;
LOCATION '/user/tables/students';

Step 2: Loading the Data
•	LOAD DATA LOCAL INPATH
'/home/Hadoop/student.txt' OVERWRITE INTO TABLE student_external

Step 3: Prepare the Data File
•	sudo nano countries.csv

Step 4: Import the File to HDFS
•	Create an HDFS directory:
o	hdfs dfs -mkdir [hdfs-directory-name]
•	Import the CSV file into HDFS:
o	hdfs dfs -put [original-file-location] [hdfs-directory-name]
•	Use the -ls command to verify that the file is in the HDFS folder:
o	hdfs dfs -ls [hdfs-directory-name]

Step 5: Create an External Table
•	To verify that the external table creation:
o	select * from [external-table-name];
•	If you wish to create a managed table using the data from an external table, type:
 


o	create table if not exists [managed-table-name](
[column1-name] [column1-type], [column2-name] [var2-name], ...) comment '[comment]';
•	import the data from the external table:
o	insert overwrite table [managed-table-name] select * from [external-table-name];
•	Verify that the data is successfully inserted into the managed table:
o	select * from [managed-table-name];

11-Step 1: Writing a Hive script.  
To write the Hive Script the file should be saved with .sql extension. Open a terminal and give the following 
command to create a Hive Script.  
Command:  
sudo gedit sample.sql  
On executing the above command, it will open this file in gedit  
1. Create the Data to store into the Table.  
To load the data into the table first we need to create an input file which contains the records that need to 
be inserted in the table.  
Let us create an input file. Command: sudo gedit input.txt 
      Edit the contents into it you want to store into table 
 
2. Creating the Table in Hive:  
Command: create table product ( productid: int, productname: string, price: float, category: string) rows 
format delimited fields terminated by „,‟ ;  
Here, product is the table name and { productid, productname, price, category} are the columns of this 
table.  
Fields terminated by „,‟ indicate that the columns in the input file are separated by the symbol „,‟. By 
default the records in the input file are separated by a new line.  
3. Describing the Table: Command: describe product  
4. Retrieving the Data:  
To retrieve the data, the select command is used. Command: Select * from product;  
The above command is used to retrieve the value of all the columns present in the table. Now, we are 
done with writing the Hive script. The file sample.sql can now be saved.  
       Step 2: Running the Hive Script  
The following is the command to run the Hive script:  
Command: hive –f /home/sample.sql  
          While executing the script, make sure that the entire path of the location of the Script file is present. 








