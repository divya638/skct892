1.
import numpy as np
from scipy.spatial.distance import euclidean
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr

A = np.array([3, 5, 2, 7])
B = np.array([4, 5, 6, 8])

def euclidean_distance(A, B):
    return euclidean(A, B)

def cosine_sim(A, B):
    A = A.reshape(1, -1)
    B = B.reshape(1, -1)
    return cosine_similarity(A, B)[0][0]

def jaccard_similarity(A, B):
    A_set = set(A)
    B_set = set(B)
    return len(A_set & B_set) / len(A_set | B_set)

def pearson_corr(A, B):
    return pearsonr(A, B)[0]

print("Euclidean Distance:", euclidean_distance(A, B))
print("Cosine Similarity:", cosine_sim(A, B))
print("Jaccard Similarity:", jaccard_similarity(A, B))
print("Pearson Correlation:", pearson_corr(A, B))

OUTPUT:
Euclidean Distance: 4.242640687119285
Cosine Similarity: 0.9480257123538014
Jaccard Similarity: 0.14285714285714285
Pearson Correlation: 0.6381723297967092

2.
import numpy as np
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data 
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
pca = PCA(n_components=1)
X_train_pca = pca.fit_transform(X_train_scaled)
print("PCA Results (Reduced to 1D):")
print("Original shape:", X_train_scaled.shape)
print("Reduced shape:", X_train_pca.shape)
print("Explained Variance Ratio:", pca.explained_variance_ratio_)
print("Total Variance Retained:", np.sum(pca.explained_variance_ratio_))
svd = TruncatedSVD(n_components=1)
X_train_svd = svd.fit_transform(X_train_scaled)
print("\nSVD Results (Reduced to 1D):")
print("Original shape:", X_train_scaled.shape)
print("Reduced shape:", X_train_svd.shape)
print("Explained Variance Ratio:", svd.explained_variance_ratio_)
print("Total Variance Retained:", np.sum(svd.explained_variance_ratio_))
OUTPUT:
[IMAGE]
Explained Variance Ratio: [0.0590205  0.04282781]
Total Explained Variance: 0.101848311361453
3.
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
data = {
    "user_id": [1, 1, 2, 2, 3, 3],
    "food": ["Pizza", "Burger", "Salad", "Soup", "Pasta", "Sandwich"],
    "description": [
        "Cheesy Italian pizza with crispy crust",
        "Juicy beef burger with fresh lettuce",
        "Healthy green salad with fresh vegetables",
        "Warm vegetable soup with rich flavors",
        "Creamy pasta with garlic and cheese",
        "Grilled sandwich with cheese and tomato"
    ]}
df = pd.DataFrame(data)
vectorizer = TfidfVectorizer(stop_words="english")
tfidf_matrix = vectorizer.fit_transform(df["description"])
user_profiles = {}
for user_id in df["user_id"].unique():
    user_indices = df[df["user_id"] == user_id].index
    profile_vector = tfidf_matrix[user_indices].mean(axis=0).A1 
    
    user_profiles[user_id] = profile_vector
# The following line was indented, causing the error. Corrected indentation:
new_items = ["Spicy tacos with fresh salsa", "Grilled chicken with herbs"]
new_item_vectors = vectorizer.transform(new_items)
for user_id, profile_vector in user_profiles.items():
    similarities = cosine_similarity(profile_vector.reshape(1, -1), new_item_vectors)
    recommended_item = new_items[np.argmax(similarities)]
    print(f"User {user_id} recommended food: {recommended_item}")

OUTPUT:
User 1 recommended food: Spicy tacos with fresh salsa
User 2 recommended food: Spicy tacos with fresh salsa
User 3 recommended food: Grilled chicken with herbs
4.
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
data = pd.DataFrame({
    'FruitID': [1, 2, 3, 4, 5],
    'Fruit': ['Apple', 'Banana', 'Orange', 'Mango', 'Pineapple'],
    'Description': [
        "A sweet and crunchy fruit, rich in fiber.",
        "A soft and sweet fruit, high in potassium.",
        "A citrus fruit, juicy and rich in vitamin C.",
        "A tropical fruit, juicy and full of flavor.",
        "A spiky tropical fruit, sweet and tangy."
    ] })
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(data['Description'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
def recommend_fruits(fruit_name, data, similarity_matrix):
    index = data[data['Fruit'] == fruit_name].index[0]
    sim_scores = list(enumerate(similarity_matrix[index]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:4]
    fruit_indices = [i[0] for i in sim_scores]
    return data['Fruit'].iloc[fruit_indices]
recommended_fruits = recommend_fruits("Apple", data, cosine_sim)
print("\nRecommended Fruits for 'Apple':")
print("--------------------------------")
for fruit in recommended_fruits:
    print(f"- {fruit}")
OUTPUT:

Recommended Fruits for 'Apple':
--------------------------------
- Orange
- Pineapple
- Banana
5.
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
data = {
    'User': ['Alice', 'Alice', 'Alice', 'Bob', 'Bob', 'Bob', 'Charlie', 'Charlie', 'Charlie'],
    'Item': ['Laptop', 'Tablet', 'Phone', 'Laptop', 'Tablet', 'Headphones', 'Phone', 'Headphones', 'Speaker'],
    'Rating': [5, 4, 3, 4, 5, 2, 4, 3, 5]}
df = pd.DataFrame(data)
user_item_matrix = df.pivot_table(index='User', columns='Item', values='Rating').fillna(0)
user_similarity = cosine_similarity(user_item_matrix)
user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)
def recommend_items(target_user, num_recommendations=2):
    similar_users = user_similarity_df[target_user].sort_values(ascending=False)[1:].index
    recommendations = {}
    for user in similar_users:
        for item in user_item_matrix.columns:
            if user_item_matrix.loc[target_user, item] == 0 and user_item_matrix.loc[user, item] > 0:
                if item not in recommendations:
                    recommendations[item] = user_item_matrix.loc[user, item]
    # Corrected indentation:
    recommended_items = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:num_recommendations]
    return [item[0] for item in recommended_items]
recommended_items = recommend_items("Alice")
print(f"Recommended items for Alice: {recommended_items}")
OUTPUT:
Recommended items for Alice: ['Speaker', 'Headphones']
5. ANOTHER CODE
!pip install pandas numpy scikit-learn

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Creating a simple user-item rating dataset
data = {
"User": ["U1", "U1", "U1", "U2", "U2", "U3", "U3", "U3", "U4", "U4", "U5", "U5", "U5"],
"Movie": ["M1", "M2", "M3", "M1", "M4", "M2", "M3", "M4", "M1", "M3", "M2", "M3", "M4"],
"Rating": [4, 5, 3, 4, 2, 5, 4, 3, 5, 3, 4, 2, 5]
}
df = pd.DataFrame(data)
print("Custom Dataset:\n", df)
//step 2
user_item_matrix = df.pivot(index="User", columns="Movie", values="Rating").fillna(0)
print("\nUser-Item Matrix:\n", user_item_matrix)
//step 3
user_similarity = cosine_similarity(user_item_matrix)
user_sim_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.
print("\nUser Similarity Matrix:\n", user_sim_df)
//step 4
                           item_similarity = cosine_similarity(user_item_matrix.T)
item_sim_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matri
print("\nItem Similarity Matrix:\n", item_sim_df)
step 6
                           def recommend_movies(user, user_item_matrix, user_sim_df, top_n=2):
similar_users = user_sim_df[user].sort_values(ascending=False).iloc[1:top_n+1].index
print(f"\nTop {top_n} similar users to {user}: {list(similar_users)}")
recommendations = user_item_matrix.loc[similar_users].mean().sort_values(ascending=False)
watched_movies = user_item_matrix.loc[user][user_item_matrix.loc[user] > 0].index
recommendations = recommendations.drop(watched_movies, errors='ignore')
return recommendations.head(3)
recommended_movies = recommend_movies("U1", user_item_matrix, user_sim_df)
print("\nRecommended Movies for U1:\n", recommended_movies)
6.
import numpy as np
import pandas as pd
from sklearn.decomposition import TruncatedSVD
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
url = "https://files.grouplens.org/datasets/movielens/ml-100k/u.data"
column_names = ["user_id", "item_id", "rating", "timestamp"]
ratings = pd.read_csv(url, sep="\t", names=column_names).drop(columns=["timestamp"])
user_item_matrix = ratings.pivot(index="user_id", columns="item_id", values="rating").fillna(0)
svd = TruncatedSVD(n_components=20)
original_matrix = svd.fit_transform(user_item_matrix)
fake_users = pd.DataFrame({
    "user_id": [944, 945, 946],  
    "item_id": [50, 50, 50],  
    "rating": [5, 5, 5]  
})
ratings = pd.concat([ratings, fake_users])
user_item_matrix = ratings.pivot(index="user_id", columns="item_id", values="rating").fillna(0)
tampered_matrix = svd.fit_transform(user_item_matrix)
print("Original Matrix Shape:", original_matrix.shape)
print("Tampered Matrix Shape:", tampered_matrix.shape)
sparse_matrix = csr_matrix(user_item_matrix)
knn = NearestNeighbors(metric="cosine", algorithm="brute")
knn.fit(sparse_matrix)
def get_recommendations(user_id):
    user_index = user_id - 1
    distances, indices = knn.kneighbors([user_item_matrix.iloc[user_index]], n_neighbors=5)
    return user_item_matrix.index[indices.flatten()[1:]]
print("Original Recommendations for User 1:", get_recommendations(1))
print("Tampered Recommendations for User 1:", get_recommendations(1))
OUTPUT:
Original Matrix Shape: (943, 20)
Tampered Matrix Shape: (946, 20)
Original Recommendations for User 1: Index([916, 864, 268, 92], dtype='int64', name='user_id')
Tampered Recommendations for User 1: Index([916, 864, 268, 92], dtype='int64', name='user_id')
7.

!pip install pandas numpy scikit-surprise
import pandas as pd

def load_product_data():
    return pd.DataFrame([
        {"id": 1, "name": "Smartphone A", "category": "Smartphone", "brand": "Brand X", "price": 800},
        {"id": 2, "name": "Smartphone B", "category": "Smartphone", "brand": "Brand Y", "price": 1200},
        {"id": 3, "name": "Smartphone C", "category": "Smartphone", "brand": "Brand X", "price": 600},
        {"id": 4, "name": "Smartphone D", "category": "Smartphone", "brand": "Brand Z", "price": 1000},
        {"id": 5, "name": "Smartphone E", "category": "Smartphone", "brand": "Brand Y", "price": 1500},
        {"id": 6, "name": "Smartphone F", "category": "Smartphone", "brand": "Brand Z", "price": 2000},
        {"id": 7, "name": "Smartphone G", "category": "Smartphone", "brand": "Brand X", "price": 1100}])

def constraint_based_recommend(products, category=None, max_price=None, brand=None):
    filtered_products = products.copy()
    if category:
        filtered_products = filtered_products[filtered_products['category'] == category]
    if max_price:
        filtered_products = filtered_products[filtered_products['price'] <= max_price]
    if brand:
        filtered_products = filtered_products[filtered_products['brand'] == brand]
    return filtered_products

products = load_product_data()
user_constraints = {"category": "Smartphone", "max_price": 1500, "brand": "Brand Y"}
recommended_products = constraint_based_recommend(products, **user_constraints)
print("Recommended Products:")
print(recommended_products[['id', 'name', 'category', 'brand', 'price']].to_string(index=False))
OUTPUT:
Recommended Products:
 id         name   category   brand  price
  2 Smartphone B Smartphone Brand Y   1200
  5 Smartphone E Smartphone Brand Y   1500
8.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
np.random.seed(42)
X = np.random.rand(1000, 2)
y = (X[:, 0] + X[:, 1] > 1).astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = Sequential([
Dense(10, activation='relu', input_shape=(2,)),
Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=20, batch_size=10, verbose=0)
y_scores = model.predict(X_test).ravel()
fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray') # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()
OUTPUT:
IMAGE
10.
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
G = nx.DiGraph()
edges = [("A", "B"), ("A", "C"), ("B", "C"), ("C", "A"), ("C", "D"), ("D", "A")]
G.add_edges_from(edges)
pagerank_scores = nx.pagerank(G, alpha=0.85) # Alpha = damping factor (default 0.85)
print("PageRank Scores:")
for node, score in pagerank_scores.items():
    print(f"{node}: {score:.4f}") # Indented this line to be part of the for loop
OUTPUT:
PageRank Scores:
A: 0.3246
B: 0.1754
C: 0.3246
D: 0.1754
//step 2 
plt.figure(figsize=(6, 4))
nx.draw(G, with_labels=True, node_color="lightblue", edge_color="gray", node_size=2000, font_size=12)
plt.show()
OUTPUT: IMAGE


