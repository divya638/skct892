1:
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import euclidean, cityblock, minkowski, jaccard
x = np.array([2, 3, 4, 5])
y = np.array([5, 6, 7, 8])
euclidean_dist = euclidean(x, y)
manhattan_dist = cityblock(x, y)
cosine_sim = cosine_similarity([x], [y])[0][0]
jaccard_sim = 1 - jaccard(x, y)
minkowski_dist = minkowski(x, y, p=3)
print("Euclidean Distance:", euclidean_dist)
print("Manhattan Distance:", manhattan_dist)
print("Cosine Similarity:", cosine_sim)
print("Jaccard Similarity:", jaccard_sim)
print("Minkowski Distance (p=3):", minkowski_dist)    

2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data
y = digits.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
pca = PCA(n_components=30)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)
plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA - Explained Variance')
plt.show()
svd = TruncatedSVD(n_components=30)
X_train_svd = svd.fit_transform(X_train_scaled)
X_test_svd = svd.transform(X_test_scaled)
print("Original Data Shape:", X_train.shape)
print("PCA Reduced Shape:", X_train_pca.shape)
print("SVD Reduced Shape:", X_train_svd.shape)
print("Explained Variance (PCA):", np.sum(pca.explained_variance_ratio_))
print("Explained Variance (SVD):", np.sum(svd.explained_variance_ratio_))

3:
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
data = {
    "user_id": [1, 1, 2, 2, 3, 3],
    "item": ["Laptop", "Smartphone", "Tablet", "Smartwatch", "Headphones", "Speaker"],
    "description": [
        "High-performance gaming laptop",
        "Latest 5G smartphone with AMOLED display",
        "Lightweight tablet with stylus support",
        "Wearable smartwatch with heart rate tracking",
        "Noise-canceling wireless headphones",
        "Bluetooth speaker with deep bass"
    ]
}
df = pd.DataFrame(data)
vectorizer = TfidfVectorizer(stop_words="english")
tfidf_matrix = vectorizer.fit_transform(df["description"])
user_profiles = {}
for user_id in df["user_id"].unique():
    user_indices = df[df["user_id"] == user_id].index
    profile_vector = tfidf_matrix[user_indices].mean(axis=0).A1 
    user_profiles[user_id] = profile_vector
new_items = ["Gaming mouse with RGB lighting", "Fitness tracker with step counter"]
new_item_vectors = vectorizer.transform(new_items)
for user_id, profile_vector in user_profiles.items():
    similarities = cosine_similarity(profile_vector.reshape(1, -1), new_item_vectors)
    recommended_item = new_items[np.argmax(similarities)]
    print(f"User {user_id} recommended item: {recommended_item}")
4:
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
       data = pd.DataFrame({
    'MovieID': [1, 2, 3, 4, 5],
    'Title': ['Inception', 'Interstellar', 'The Dark Knight', 'Memento', 'Tenet'],
    'Description': [
        "A thief who enters the dreams of others to steal their secrets.",
        "A group of explorers travel through a wormhole in space to save humanity.",
        "Batman fights crime in Gotham while facing the Joker.",
        "A man with short-term memory loss tries to uncover the truth.",
        "A secret agent must manipulate time to prevent World War III." ] })
       tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(data['Description'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
def recommend_movies(movie_title, data, similarity_matrix):
    index = data[data['Title'] == movie_title].index[0]
    sim_scores = list(enumerate(similarity_matrix[index]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:4]
    movie_indices = [i[0] for i in sim_scores]
    return data['Title'].iloc[movie_indices]
recommended_movies = recommend_movies("Inception", data, cosine_sim)
print("Movies recommended for 'Inception':")
print(recommended_movies)


6:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.sparse.linalg import svds
np.random.seed(42)
num_users = 10
num_items = 10  # Increased from 8 to 10
ratings = np.random.randint(1, 6, (num_users, num_items))  
ratings_df = pd.DataFrame(ratings, columns=[f'Item_{i+1}' for i in range(num_items)])
ratings_df.index = [f'User_{i+1}' for i in range(num_users)]
def recommend_items(rating_matrix, k=3):
    rating_matrix = rating_matrix.astype(float)
    U, sigma, Vt = svds(rating_matrix, k=k)
    sigma = np.diag(sigma)
    reconstructed_matrix = np.dot(np.dot(U, sigma), Vt)
    return reconstructed_matrix
original_recommendations = recommend_items(ratings)
fake_user = np.random.randint(1, 3, (1, num_items))
fake_user[0, 3] = 5  # Attacker targets "Item_4"
ratings_with_attack = np.vstack([ratings, fake_user])
attacked_recommendations = recommend_items(ratings_with_attack)
original_ranking = np.argsort(-original_recommendations.mean(axis=0))
attacked_ranking = np.argsort(-attacked_recommendations.mean(axis=0))
ranking_df = pd.DataFrame({
    'Item': [f'Item_{i+1}' for i in range(num_items)],
    'Rank Before Attack': np.argsort(original_ranking) + 1,
    'Rank After Attack': np.argsort(attacked_ranking) + 1
})
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
sns.heatmap(ratings, annot=True, cmap="Blues", fmt="d", xticklabels=[f'Item_{i+1}' for i in range(num_items)],
       yticklabels=[f'User_{i+1}' for i in range(num_users)])
plt.title("User-Item Ratings Before Attack")
plt.subplot(1, 2, 2)
sns.heatmap(ratings_with_attack, annot=True, cmap="Reds", fmt="d", xticklabels=[f'Item_{i+1}' for i in range(num_items)],
yticklabels=[f'User_{i+1}' for i in range(num_users)] + ["Attacker"])
plt.title("User-Item Ratings After Attack")
plt.show()
plt.figure(figsize=(10, 5))
sns.barplot(x='Item', y ='Rank Before Attack', data=ranking_df, label="Before Attack", color="blue")
sns.barplot(x='Item', y='Rank After Attack', data=ranking_df, label="After Attack", color="red", alpha=0.6)
plt.legend()
plt.title("Item Rankings Before and After Attack")
plt.ylabel("Rank (Lower is Better)")
plt.show()

5:
import numpy as np
import pandas as pd
from sklearn.decomposition import TruncatedSVD
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix
url = "https://files.grouplens.org/datasets/movielens/ml-100k/u.data"
column_names = ["user_id", "item_id", "rating", "timestamp"]
ratings = pd.read_csv(url, sep="\t", names=column_names)
ratings = ratings.drop(columns=["timestamp"])
user_item_matrix = ratings.pivot(index="user_id", columns="item_id", values="rating").fillna(0)
svd = TruncatedSVD(n_components=20)
reduced_matrix = svd.fit_transform(user_item_matrix)
sparse_matrix = csr_matrix(user_item_matrix)
knn = NearestNeighbors(metric="cosine", algorithm="brute")
knn.fit(sparse_matrix)
def get_similar_users(user_id, n_neighbors=5):
    user_index = user_id - 1
    distances, indices = knn.kneighbors([user_item_matrix.iloc[user_index]], n_neighbors=n_neighbors+1)
    return user_item_matrix.index[indices.flatten()[1:]]
similar_users = get_similar_users(1)
print("Users similar to user 1:", similar_users.tolist() 

7:
import pandas as pd
import numpy as np
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split, cross_validate
import os
def preprocess_data(filename):
    df = pd.read_csv(filename)
    df['AvgTicketPrice'] = df['AvgTicketPrice'].replace('[\$,]', '', regex=True).astype(float)
    df['user_id'] = df['Origin'] + '_' + df['OriginAirportID']
    df['item_id'] = df['Dest'] + '_' + df['DestAirportID']
    return df[['user_id', 'item_id', 'AvgTicketPrice']]
def train_model(df):
    reader = Reader(rating_scale=(df['AvgTicketPrice'].min(), df['AvgTicketPrice'].max()))
    data = Dataset.load_from_df(df[['user_id', 'item_id', 'AvgTicketPrice']], reader)
    trainset, testset = train_test_split(data, test_size=0.2)
    model = SVD()
    model.fit(trainset)
    cross_validate(model, data, cv=5, verbose=True)
    return model
def recommend(model, user, df, top_n=5):
    unique_items = df['item_id'].unique()
    predictions = [(item, model.predict(user, item).est) for item in unique_items]
    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]
    return recommendations
def upload_file():
    from google.colab import files
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]
    return filename
dataset_filename = "flights.csv"
if not os.path.exists(dataset_filename):
    dataset_filename = upload_file()
df = preprocess_data(dataset_filename)
model = train_model(df)
sample_user = df['user_id'].iloc[0]
recommendations = recommend(model, sample_user, df)
print(f"Recommendations for user {sample_user}: {recommendations}")

8:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
np.random.seed(42)
X = np.random.rand(1000, 2)

y = (X[:, 0] + X[:, 1] > 1).astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = Sequential([
    Dense(12, activation='relu', input_shape=(2,)),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=25, batch_size=10, verbose=0)
y_scores = model.predict(X_test).ravel()
fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(7, 7))
plt.plot(fpr, tpr, color='red', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='black')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

9:
import networkx as nx
import matplotlib.pyplot as plt
G = nx.DiGraph()
       edges = [
    ('User1', 'ItemA'), ('User1', 'ItemB'), ('User2', 'ItemA'),
    ('User2', 'ItemC'), ('User3', 'ItemB'), ('User3', 'ItemD'),
    ('User4', 'ItemC'), ('User4', 'ItemD'), ('User5', 'ItemE')
]
G.add_edges_from(edges)
pagerank_values = nx.pagerank(G, alpha=0.85)
       print("PageRank Scores (Recommendation Scores):")
for node, score in pagerank_values.items():
    print(f"{node}: {score:.4f}")
plt.figure(figsize=(8, 6))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=12)
plt.title("Graph Representation of User-Item Interactions")
plt.show()





